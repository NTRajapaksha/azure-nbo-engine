{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5bd21d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 1: DATA GENERATION\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from faker import Faker\n",
    "\n",
    "print(\"Initializing Data Generator...\")\n",
    "fake = Faker()\n",
    "Faker.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "NUM_CUSTOMERS = 10_000\n",
    "NUM_INTERACTIONS = 100_000\n",
    "PRODUCTS = [\n",
    "    {'id': 101, 'name': 'Super Saver Account', 'category': 'Savings'},\n",
    "    {'id': 102, 'name': 'Gold Credit Card', 'category': 'Credit'},\n",
    "    {'id': 103, 'name': 'Platinum Miles Card', 'category': 'Credit'},\n",
    "    {'id': 104, 'name': 'Home Improvement Loan', 'category': 'Loan'},\n",
    "    {'id': 105, 'name': 'Auto Loan', 'category': 'Loan'},\n",
    "    {'id': 106, 'name': 'Crypto Wallet', 'category': 'Investment'},\n",
    "    {'id': 107, 'name': 'Retirement Fund', 'category': 'Investment'},\n",
    "    {'id': 108, 'name': 'Student Checking', 'category': 'Savings'},\n",
    "]\n",
    "\n",
    "# --- 1. GENERATE CUSTOMERS ---\n",
    "customers = []\n",
    "for i in range(NUM_CUSTOMERS):\n",
    "    customers.append({\n",
    "        'customer_id': i + 1,\n",
    "        'name': fake.name(),\n",
    "        'age': random.randint(18, 75),\n",
    "        'income': random.randint(20000, 150000),\n",
    "        'credit_score': random.randint(300, 850)\n",
    "    })\n",
    "df_customers = pd.DataFrame(customers)\n",
    "\n",
    "# --- 2. GENERATE INTERACTIONS ---\n",
    "interactions = []\n",
    "for _ in range(NUM_INTERACTIONS):\n",
    "    cust = df_customers.sample(1).iloc[0]\n",
    "    \n",
    "    # Simple logic: Wealthy -> Platinum/Crypto; Young -> Student/Saver\n",
    "    if cust['income'] > 100000:\n",
    "        prod = random.choice([p for p in PRODUCTS if p['id'] in [103, 106, 107]])\n",
    "    elif cust['age'] < 25:\n",
    "        prod = random.choice([p for p in PRODUCTS if p['id'] in [108, 101]])\n",
    "    else:\n",
    "        prod = random.choice(PRODUCTS)\n",
    "        \n",
    "    # Implicit Rating: 1 = View, 5 = Purchase\n",
    "    event_type = np.random.choice(['view', 'purchase'], p=[0.7, 0.3])\n",
    "    rating = 5 if event_type == 'purchase' else 1\n",
    "    \n",
    "    interactions.append({\n",
    "        'customer_id': cust['customer_id'],\n",
    "        'product_id': prod['id'],\n",
    "        'rating': rating,\n",
    "        'timestamp': fake.date_time_between(start_date='-1y', end_date='now')\n",
    "    })\n",
    "\n",
    "df_interactions = pd.DataFrame(interactions)\n",
    "df_products = pd.DataFrame(PRODUCTS)\n",
    "\n",
    "# --- 3. SAVE TO SPARK TABLES (The \"Lakehouse\" Layer) ---\n",
    "# Convert Pandas -> Spark DataFrame\n",
    "spark_cust = spark.createDataFrame(df_customers)\n",
    "spark_inter = spark.createDataFrame(df_interactions)\n",
    "spark_prod = spark.createDataFrame(df_products)\n",
    "\n",
    "# Save as global tables so we can query them with SQL or Spark later\n",
    "spark_cust.write.mode(\"overwrite\").saveAsTable(\"customers\")\n",
    "spark_inter.write.mode(\"overwrite\").saveAsTable(\"interactions\")\n",
    "spark_prod.write.mode(\"overwrite\").saveAsTable(\"products\")\n",
    "\n",
    "print(f\"SUCCESS: Generated {NUM_CUSTOMERS} customers and saved to Spark Tables.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23b8562",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 2: DISTRIBUTED TRAINING (ALS) - FIXED\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col, explode\n",
    "\n",
    "# 1. Load Data from Tables\n",
    "df_interactions = spark.table(\"interactions\")\n",
    "df_products = spark.table(\"products\")\n",
    "\n",
    "# --- FIX: Rename 'id' to 'product_id' so the join works later ---\n",
    "df_products = df_products.withColumnRenamed(\"id\", \"product_id\")\n",
    "\n",
    "# 2. Prep Data (Casting)\n",
    "training_data = df_interactions.select(\n",
    "    col(\"customer_id\").cast(\"integer\"),\n",
    "    col(\"product_id\").cast(\"integer\"),\n",
    "    col(\"rating\").cast(\"float\")\n",
    ")\n",
    "(train, test) = training_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# 3. Train ALS Model\n",
    "print(\"Training Distributed ALS Model...\")\n",
    "als = ALS(\n",
    "    maxIter=10, \n",
    "    regParam=0.1, \n",
    "    userCol=\"customer_id\", \n",
    "    itemCol=\"product_id\", \n",
    "    ratingCol=\"rating\",\n",
    "    coldStartStrategy=\"drop\"\n",
    ")\n",
    "model = als.fit(train)\n",
    "\n",
    "# 4. Evaluate (RMSE)\n",
    "predictions = model.transform(test)\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Model Trained. Root-mean-square error = {rmse:.4f}\")\n",
    "\n",
    "# 5. Generate Recommendations (NBO)\n",
    "print(\"Generating 'Next Best Offer' for all users...\")\n",
    "user_recs = model.recommendForAllUsers(3)\n",
    "\n",
    "# 6. Formatting\n",
    "user_recs_exploded = user_recs.select(\n",
    "    col(\"customer_id\"), \n",
    "    explode(\"recommendations\").alias(\"rec\")\n",
    ").select(\n",
    "    col(\"customer_id\"),\n",
    "    col(\"rec.product_id\"),\n",
    "    col(\"rec.rating\").alias(\"prediction_score\")\n",
    ")\n",
    "\n",
    "# 7. Join with Product Names (Now this will work!)\n",
    "final_nbo = user_recs_exploded.join(df_products, \"product_id\") \\\n",
    "    .select(\"customer_id\", \"product_id\", \"name\", \"category\", \"prediction_score\")\n",
    "\n",
    "# Show a sample\n",
    "display(final_nbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd12112",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CELL 3: EXPORT TO CSV (Robust Version)\n",
    "\n",
    "# 1. Define Path using the DBFS protocol (stable)\n",
    "save_path = \"dbfs:/FileStore/nbo_export_csv\"\n",
    "\n",
    "# 2. Write the single CSV file\n",
    "# We use coalesce(1) to merge data into one file\n",
    "print(\"Writing CSV to DBFS...\")\n",
    "final_nbo.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(save_path)\n",
    "\n",
    "# 3. Find the filename using dbutils (Native Tool) instead of 'os'\n",
    "# This lists files in that folder\n",
    "files = dbutils.fs.ls(save_path)\n",
    "csv_name = [x.name for x in files if x.name.endswith(\".csv\")][0]\n",
    "\n",
    "# 4. Generate Download Link\n",
    "# We construct the URL dynamically\n",
    "workspace_url = spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "download_url = f\"https://{workspace_url}/files/nbo_export_csv/{csv_name}\"\n",
    "\n",
    "print(f\"SUCCESS! Download your NBO Data here:\")\n",
    "print(\"-\" * 30)\n",
    "print(download_url)\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
